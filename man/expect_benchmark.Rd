% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/expect-benchmark.R
\name{expect_benchmark}
\alias{expect_benchmark}
\title{Benchmark testing}
\usage{
expect_benchmark(benchmark)
}
\arguments{
\item{benchmark}{benchmark code to evaluate that has been created with
\code{\link[bench:mark]{bench::mark()}}.}
}
\description{
Benchmark tests are designed to be similar in spirit to to unit tests,
specifically to \code{\link[testthat:expect_snapshot]{testthat::expect_snapshot()}} tests. The benchmark is
stored in a separate file, managed by benchthat. Benchmark tests are useful
for when you want to evaluate the speed of your code, and want to be able to
freely change an existing implementation without going through the process
of copying a function to compare it's speed.

\code{expect_benchmark()} takes a benchmark created by \code{\link[bench:mark]{bench::mark()}}, and
records the results of the time taken to execute the code.
}
\section{Workflow}{

The first time you run a benchmark expectation it will take the \code{benchmark}
result and record them in \verb{benchmarks/benchthat/_benches/\{test\}.rds}.
Each benchmark run gets its own benchmark file, i.e., one function per
\code{expect_benchmark()}.

It's important to review the benchmark files, and then commit them to git.
You can review a benchmark with \code{\link[=benchmark_review]{benchmark_review()}}. This displays the
differences between a new implementation and the current one, and gives you
the capacity to accept a new benchmark code. You can also generate this
with \code{\link[=benchmark_report]{benchmark_report()}}.

On subsequent runs, the result of \code{benchmark} will be compared to the
benchmark file stored on disk. If it's any different, the expectation will
fail,  and a new file \verb{_benches/\{test\}.new.csv} will be created. If the
change was deliberate, you can approve the change with \code{\link[=benchmark_accept]{benchmark_accept()}}.
}

